{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP: Tokenization & Stopword Removal using spaCy. This is advanced as compared to NLTK\n",
    "# While NLTK’s word_tokenize would simply split on whitespace and punctuation, spaCy’s hybrid approach keeps “spaCy—it's” as [\"spaCy\", \"—\", \"it\", \"'s\"] (or similar), and correctly preserves URLs, emoticons, and multi‑word tokens you’ve configured.\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "nlp = English()  # blank pipeline\n",
    "\n",
    "#  Step 1: View default infix patterns\n",
    "#infix_patterns = nlp.Defaults.infixes\n",
    "\n",
    "#  Step 2: Compile infix regex from patterns\n",
    "#import re\n",
    "#infix_re = re.compile(\"|\".join(infix_patterns))\n",
    "\n",
    "#   Step 2 (Optional): Modify infix rules (e.g., keep hyphenated words intact)\n",
    "# This is faster code as compared to the above one. Now it generate less token and faster as compard to NLTK \n",
    "infixes = [x for x in nlp.Defaults.infixes if x != r\"(?<=[A-Za-z0-9])[-–~](?=[A-Za-z0-9])\"]\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "\n",
    "text = \"I'm loving spaCy—it's awesome! Visit https://example.com.\"\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
